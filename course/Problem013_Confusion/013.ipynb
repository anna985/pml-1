{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "Python 3.8.2 64-bit ('pml')",
      "display_name": "Python 3.8.2 64-bit ('pml')",
      "metadata": {
        "interpreter": {
          "hash": "a4c9474aacc61cf72d0f1c29f4a339e5d6b2171c287541cfd684cf058783219b"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "colab": {
      "name": "Copy of 013.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna985/pml-1/blob/master/course/Problem013_Confusion/013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "jC31t7FuiZG9"
      },
      "source": [
        "# How to evaluate following systems?\n",
        "\n",
        "## Credit Approval (Yes/No)\n",
        "## Covid detection (Positive/Negative)\n",
        "## Court guilty judgement (Guilty/Not Guilty)\n",
        "## Spam detection (Spam/Ham)\n",
        "## Anomaly detection (Anomaly/Normal)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzT7x3rfiZG-"
      },
      "source": [
        "# Some classification process under study generated following labels \n",
        "Y =  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# 2 classification models trying to approximate above process produced following end predictions\n",
        "Y_hat1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "Y_hat2 = [1, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "#TBD Which is the better model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ON5DE6viZHC"
      },
      "source": [
        "# TBD: Compute accuracy of the models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyfaFaWRiZHF"
      },
      "source": [
        "# TBD: Compute confusion matrix for the models\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvFKlGx3iZHI"
      },
      "source": [
        "# TBD: Compute precision, recall and f1 score for the models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep9fpJSmiZHK"
      },
      "source": [
        "# Note: Changed data slightly from original problem, as that was sort of edge case.\n",
        "# Feel free to apply this solution on the original data as well \n",
        "\n",
        "# Some classification process under study generated following labels \n",
        "Y =  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "\n",
        "# 2 classification models trying to approximate above process produced following end predictions\n",
        "Y_hat1 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "Y_hat2 = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "#TBD Which is the better model\n",
        "\n",
        "#Solution:\n",
        "# We have 2 output classes 0 and 1,\n",
        "# from accuracy perspective Model1 looks better model, but if you notice, output  \n",
        "# class distribution is very skewed (imbalanced), 1 is a rare class. So even a dumb model which just \n",
        "# produces zeros will have good accuracy without trying anything. Model2 overall has got \n",
        "# more wrong but it got all of the rare class right, so in some scenarios it might be a better model \n",
        "# In case of skewed classes, accuracy is not a good metrics to rate model as we will \n",
        "# see below precision/recall and f1 score are more indicative of model performance."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gej3N3gvWr0z",
        "outputId": "34b74bb5-d336-40b1-a8da-0b559748e6ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TBD: Compute accuracy of the models\n",
        "\n",
        "#Solution\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy1 = accuracy_score(Y, Y_hat1)\n",
        "accuracy2 = accuracy_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"accuracy of model1 = {accuracy1}. {accuracy1*100}% accurate\")\n",
        "print(f\"accuracy of model2 = {accuracy2}. {accuracy2*100}% accurate\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of model1 = 0.9. 90.0% accurate\n",
            "accuracy of model2 = 0.8. 80.0% accurate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlEKfmCrW1ep",
        "outputId": "56a0e37f-0163-459b-8f9a-5ca0605510a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "# TBD: Compute confusion matrix for the models\n",
        "\n",
        "#Solution\n",
        "\n",
        "'''\n",
        "Positive/Negative refer to class\n",
        "    1 is Positive Class (Usually rare class should be kept positive as metrics like precision, recall are defined from perspective of positive class)\n",
        "    0 is Negative Class\n",
        "\n",
        "True Positives (TP)  = Classified Positive and are Correct\n",
        "False Positives (FP) = Classified Positive and are Incorrect\n",
        "False Negatives (FN) = Classified Negative and are Incorrect\n",
        "True Negatives (TN)  = Classified Negative and are Correct\n",
        "\n",
        "True/False refers to classification\n",
        "    True means classified correctly\n",
        "    False means classified incorrectly\n",
        "\n",
        "+-------------------------------------------------+\n",
        "|                 Confusion Matrix                |\n",
        "+------------+-----------------+------------------+\n",
        "|            | Actual Positive | Actual Negative  |\n",
        "+------------+-----------------+------------------+\n",
        "| Predicted  | True Positive   | False Positive   |\n",
        "| Positive   |                 |                  |\n",
        "+------------+-----------------+------------------+\n",
        "| Predicted  | False Negative  | True Negative    |\n",
        "| Negative   |                 |                  |\n",
        "+------------+-----------------+------------------+\n",
        "\n",
        "'''\n",
        "# \n",
        "# Y      = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "# Y_hat1 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "confusion1 = np.array([\n",
        "    [1, 0],\n",
        "    [1, 8]\n",
        "])\n",
        "\n",
        "# \n",
        "# Y      = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "# Y_hat2 = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "confusion2 = np.array([\n",
        "    [2, 2],\n",
        "    [0, 6]\n",
        "])\n",
        "\n",
        "print(f\"confusion1 = \\n{confusion1}\")\n",
        "print(f\"confusion2 = \\n{confusion2}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion1 = \n",
            "[[1 0]\n",
            " [1 8]]\n",
            "confusion2 = \n",
            "[[2 2]\n",
            " [0 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG__V3-JW9GH",
        "outputId": "a9ef240a-49b4-4558-c4b1-a5fcbf17c223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TBD: Compute precision, recall and f1 score for the models\n",
        "\n",
        "#Solution (Precision, Recall, and F1 score Conceptually)\n",
        "\n",
        "# Precision = TP/(TP + FP)\n",
        "# Precision means what fraction of class we are classifying positive is actually positive\n",
        "precision1 = 1/(1 + 0)\n",
        "print(f\"precision1 = {precision1}\")\n",
        "precision2 = 2/(2 + 2)\n",
        "print(f\"precision2 = {precision2}\")\n",
        "\n",
        "# Recall = TP/(TP + FN)\n",
        "# Recall means what fraction of total positive class we are classifying as positive\n",
        "recall1 = 1/(1+1)\n",
        "print(f\"recall1 = {recall1}\")\n",
        "recall2 = 2/(2+0)\n",
        "print(f\"recall2 = {recall2}\")\n",
        "\n",
        "\n",
        "# Usually ML systems tradeoff between precision and recall, higher precision\n",
        "# may lead to lower recall and vise versa. for e.g in a Serious disease detector\n",
        "# we may favour recall over precision as we did not want to miss any case.\n",
        "# Some judicial systems heavily tilt towards precision at the cost of recall\n",
        "# It doesnt matter if 100 guilty are acquitted, but not one innocent should be punished.\n",
        "# Similarly if designing a anomaly detection system you would favour recall for mission\n",
        "# critical system, but for normal system you might favour precision to avoid waking\n",
        "# engineers at night\n",
        "\n",
        "\n",
        "# F1 score, Usually in case of skewed classes, metrics like accuracy are not\n",
        "# trustworthy, precision, recall serve the purpose but they are 2 metrics\n",
        "# and usually for evaluation of ML systems its reccommended to have single value metrics\n",
        "# F1 combines both precision and recall, its a harmonic mean of precision and recall\n",
        "\n",
        "# f1 = 2 * precision * recall /(precision + recall)\n",
        "\n",
        "def f1(precision, recall):\n",
        "    return 2*precision* recall/(precision + recall)\n",
        "\n",
        "f1_1 = f1(precision1, recall1)\n",
        "print(f\"F1 score of model1 = {f1_1}\")\n",
        "f1_2 = f1(precision2, recall2)\n",
        "print(f\"F1 score of model2 = {f1_2}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision1 = 1.0\n",
            "precision2 = 0.5\n",
            "recall1 = 0.5\n",
            "recall2 = 1.0\n",
            "F1 score of model1 = 0.6666666666666666\n",
            "F1 score of model2 = 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeMj5JXVXEW4",
        "outputId": "6f9694cf-7c3a-4960-9b8d-6717438de4e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Solution: using sklearn\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision1 = precision_score(Y, Y_hat1) \n",
        "precision2 = precision_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"precision1 = {precision1}\")\n",
        "print(f\"precision2 = {precision2}\")\n",
        "\n",
        "recall1 = recall_score(Y, Y_hat1)\n",
        "recall2 = recall_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"recall1 = {recall1}\")\n",
        "print(f\"recall2 = {recall2}\")\n",
        "\n",
        "f1_1 = f1_score(Y, Y_hat1)\n",
        "f1_2 = f1_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"F1 score of model1 = {f1_1}\")\n",
        "print(f\"F1 score of model2 = {f1_2}\")\n",
        "\n",
        "# Precision, Recall and F1 are defined around notion of positive class\n",
        "# 1 is default positive class, but if you want another label to be positive class\n",
        "# you can override it by specifying pos_label argument\n",
        "\n",
        "model1_precision_wrt_0_as_positive_class = precision_score(Y, Y_hat1, pos_label=0)\n",
        "print(f\"model1 precision with 0 as positive class = {model1_precision_wrt_0_as_positive_class}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision1 = 1.0\n",
            "precision2 = 0.5\n",
            "recall1 = 0.5\n",
            "recall2 = 1.0\n",
            "F1 score of model1 = 0.6666666666666666\n",
            "F1 score of model2 = 0.6666666666666666\n",
            "model1 precision with 0 as positive class = 0.8888888888888888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foX80fA1XVgE",
        "outputId": "f8002bcc-82e5-4090-e50a-3d4fd4b0519f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Note: Confusion matrix using sklearn has Actual Class in Rows and Predicted class in columns (i.e axes are other way round)\n",
        "'''\n",
        "+-----------------------------------------------------+\n",
        "|              Confusion Matrix (sklearn)             |\n",
        "+----------+--------------------+---------------------+\n",
        "|          | Predicted Positive | Predicted Negative  |\n",
        "+----------+--------------------+---------------------+\n",
        "| Actual   | True Positive      | False Negative      |\n",
        "| Positive |                    |                     |\n",
        "+----------+--------------------+---------------------+\n",
        "| Actual   | False Positive     | True Negative       |\n",
        "| Negative |                    |                     |\n",
        "+----------+--------------------+---------------------+\n",
        "'''\n",
        "#Positive and Negative Labels can be specified in labels arguments \n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion1_sklearn = confusion_matrix(Y, Y_hat1, labels = [1, 0]) #Order in which we specify labels will decide which will be positive class, [1, 0] means 1 is positive class, 0 is negative class\n",
        "confusion2_sklearn = confusion_matrix(Y, Y_hat2, labels = [1, 0])\n",
        "print(f\"confusion1_sklearn = \\n{confusion1_sklearn}\")\n",
        "print(f\"confusion2_skelarn = \\n{confusion2_sklearn}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion1_sklearn = \n",
            "[[1 1]\n",
            " [0 8]]\n",
            "confusion2_skelarn = \n",
            "[[2 0]\n",
            " [2 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}