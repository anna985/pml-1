{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('pml': venv)",
      "metadata": {
        "interpreter": {
          "hash": "a4c9474aacc61cf72d0f1c29f4a339e5d6b2171c287541cfd684cf058783219b"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "colab": {
      "name": "Copy of 025.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna985/pml-1/blob/master/course/Problem025_NNScratch/025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBsvW5vLi4LQ"
      },
      "source": [
        "## TBD Code reading, Below is a from-scratch  3 layer neural network. Main training code is in the *fit* method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voX-n12Wi4LR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKItxQ3Ui4LR"
      },
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (relu Function) x if x > 0 else x\n",
        "\n",
        "    :param Z: input tensor\n",
        "    :return:  Relu output\n",
        "    \"\"\"\n",
        "    A = np.maximum(0, Z)\n",
        "    return A\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Sigmoid Function\n",
        "    :param Z: input tensor\n",
        "    :return: Sigmoid output\n",
        "    \"\"\"\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Derivative of relu function\n",
        "\n",
        "    :param dA:  How cost changes with activation ( dJ_by_dA )\n",
        "    :param Z:\n",
        " \n",
        "    :return:  How cost changes with weighted sum (Z) ( dJ_by_dZ )\n",
        "    \"\"\"\n",
        "\n",
        "    dZ = np.array(dA, copy=True) \n",
        "\n",
        "    dZ[Z <= 0] = 0\n",
        "\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Derivative of sigmoid function\n",
        "\n",
        "    :param dA: How cost changes with activation ( dJ_by_dA )\n",
        "    :param Z:\n",
        "    \n",
        "    :return: How cost changes with weighted sum (Z) ( dJ_by_dZ )\n",
        "    \"\"\"\n",
        "\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    dZ = dA * s * (1 - s)\n",
        "\n",
        "    return dZ\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzds3WhKi4LR"
      },
      "source": [
        "def init_weights(n_x, n_h1, n_h2, n_y):\n",
        "    \"\"\"\n",
        "\n",
        "    Initialize weights tensor\n",
        "\n",
        "    :param n_x: size of the input layer\n",
        "    :param n_h1: size of the hidden layer 1\n",
        "    :param n_h2: size of the hidden layer 2\n",
        "    :param n_y: size of the output layer\n",
        "\n",
        "    :return:     weights -- python dictionary containing initialized weights:\n",
        "                    W1 -- weight matrix of shape (n_h1, n_x)\n",
        "                    b1 -- bias vector of shape (n_h1, 1)\n",
        "                    W2 -- weight matrix of shape (n_h2, n_h1)\n",
        "                    b2 -- bias vector of shape (n_h2, 1)\n",
        "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
        "                    b3 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = np.random.randn(n_h1, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h1, 1))\n",
        "    W2 = np.random.randn(n_h2, n_h1) * 0.01\n",
        "    b2 = np.zeros((n_h2, 1))\n",
        "    W3 = np.random.randn(n_y, n_h2) * 0.01\n",
        "    b3 = np.zeros((n_y, 1))\n",
        "\n",
        "    weights = { \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 }\n",
        "\n",
        "    return weights\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kVsSRTsi4LR"
      },
      "source": [
        "def step_forward(A, W, b, activation_function):\n",
        "    \"\"\"\n",
        "    Step forward one layer in a deep neural network\n",
        "\n",
        "    :param A: Input tensor for the layer\n",
        "    :param W: Weight tensor for the layer\n",
        "    :param b: Bias tensor for the layer\n",
        "    :param activation_function: Activation function to be applied to the layer\n",
        "\n",
        "    :return: Output Activation tensor and Weighted sum\n",
        "    \"\"\"\n",
        "\n",
        "    Z = np.matmul(W, A) + b\n",
        "    A_next = activation_function(Z)\n",
        "\n",
        "    return A_next, Z\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "\n",
        "    Cross Entropy Error (Cost) between Y-Hat and Y  (Y-Hat is activation of Lth Layer hence AL)\n",
        "\n",
        "    :param AL: Activation of Lth Layer i.e Y-Hat\n",
        "    :param Y: Actual Labels (Ground Truth)\n",
        "\n",
        "    :return: Cost (How close are we?)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))) / m\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def step_backward(dA, Z, A_prev, W, b, activation_function_backward):\n",
        "    \"\"\"\n",
        "    Step back one layer in deep neural network,\n",
        "    i.e Find out how much adjustment is needed in weights in this layer\n",
        "\n",
        "    :param dA: How Final cost changes with Activation of this layer ( dJ_by_dA )\n",
        "    :param Z: Weighted Sum from this layer\n",
        "    :param A_prev: Incoming activations to this layer\n",
        "    :param W: Weights of this layer\n",
        "    :param b: Bias of this layer\n",
        "    :param activation_function_backward:\n",
        "    \n",
        "    :return:\n",
        "        dA_prev : How cost changes with activation from prev layer ( dJ_by_dA_prev), need this for further back propagation of cost to previous layer\n",
        "        dW: How cost changes with weights of this layer (our prize). (dJ_by_dW)\n",
        "        db: How cost changes with weights of this layer (dJ_by_db)\n",
        "    \"\"\"\n",
        "\n",
        "    dZ = activation_function_backward(dA, Z)\n",
        "\n",
        "    N = A_prev.shape[1]\n",
        "\n",
        "    dW = np.matmul(dZ, A_prev.transpose()) / N\n",
        "    db = np.sum(dZ, axis=1, keepdims=True) / N\n",
        "    dA_prev = np.matmul(W.transpose(), dZ)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def update_weights(weights, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update weights across all layers\n",
        "    After our back-propogation we know how much weight & bias adjustment is needed\n",
        "    in each layer, This function performs that update.\n",
        "\n",
        "    :param weights: Original weights\n",
        "    :param grads: Changes that needs to be made to weights (dW and db s)\n",
        "    :param learning_rate: learning rate of algorithm. (a step length in gradient descent)\n",
        "\n",
        "    :return: updated weights\n",
        "    \"\"\"\n",
        "    L = len(weights)//2    # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        weights[\"W\" + str(l + 1)] = weights[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        weights[\"b\" + str(l + 1)] = weights[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
        "\n",
        "    return weights\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmC3xrOpi4LR"
      },
      "source": [
        "## Fit (Training of Neural network using back propagation and gradient descent )\n",
        "<img src=\"https://github.com/rawata/pml/blob/master/img/nn_training_loop.png?raw=1\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDJP7Bvgi4LR"
      },
      "source": [
        "\n",
        "\n",
        "def fit(X, Y, layers_dims, learning_rate=0.0075, num_iterations=10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Trains a 3 layer neural network\n",
        "\n",
        "    :param X: input features\n",
        "    :param Y: output labels\n",
        "    :param layers_dims: number of nodes in each layer ( inputs, number of neurons in each hidden layer, output)\n",
        "    :param learning_rate: \n",
        "    :param num_iterations:\n",
        "    :param print_cost: \n",
        "\n",
        "    :return: Weights and Bias of the trained network. \n",
        "\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []  # to keep track of the cost\n",
        "    m = X.shape[1]  # number of examples\n",
        "    \n",
        "    (n_x, n_h1, n_h2, n_y) = layers_dims\n",
        "\n",
        "    weights = init_weights(n_x, n_h1, n_h2, n_y)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: (LINEAR -> RELU) * 2 -> LINEAR -> SIGMOID.\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from weights\n",
        "        W1 = weights[\"W1\"]\n",
        "        b1 = weights[\"b1\"]\n",
        "        W2 = weights[\"W2\"]\n",
        "        b2 = weights[\"b2\"]\n",
        "        W3 = weights[\"W3\"]\n",
        "        b3 = weights[\"b3\"]\n",
        "\n",
        "\n",
        "        A1, Z1  = step_forward(X, W1, b1, relu)\n",
        "        A2, Z2  = step_forward(A1, W2, b2, relu)\n",
        "        A3, Z3  = step_forward(A2, W3, b3, sigmoid)\n",
        "\n",
        "        cost = compute_cost(A3, Y)\n",
        "\n",
        "        # Initializing backward propagation\n",
        "        dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3))\n",
        "\n",
        "        # gradient descent through back propagation\n",
        "        dA2, dW3, db3 = step_backward(dA3, Z3, A2, W3, b3, sigmoid_backward)\n",
        "        dA1, dW2, db2 = step_backward(dA2, Z2, A1, W2, b2, relu_backward)\n",
        "        dA0, dW1, db1 = step_backward(dA1, Z1, X, W1, b1,  relu_backward)\n",
        "\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        grads['dW3'] = dW3\n",
        "        grads['db3'] = db3\n",
        "\n",
        "        # Update weights.\n",
        "        weights = update_weights(weights, grads, learning_rate)\n",
        "\n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return weights\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvHQVBwi4LR"
      },
      "source": [
        "## Predict and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pCgyth_i4LR"
      },
      "source": [
        "def predict(X, weights):\n",
        "    \"\"\"\n",
        "    Given weights of deep neural network, predict the output for X\n",
        "\n",
        "    :param X: input tensor\n",
        "    :param weights: Weights and Bias of the network\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    W1 = weights[\"W1\"]\n",
        "    b1 = weights[\"b1\"]\n",
        "    W2 = weights[\"W2\"]\n",
        "    b2 = weights[\"b2\"]\n",
        "    W3 = weights[\"W3\"]\n",
        "    b3 = weights[\"b3\"]\n",
        "\n",
        "    A1, Z1 = step_forward(X, W1, b1, relu)\n",
        "    A2, Z2 = step_forward(A1, W2, b2, relu)\n",
        "    A3, Z3 = step_forward(A2, W3, b3, sigmoid)\n",
        "    A3 = np.around(A3)\n",
        "    return A3\n",
        "\n",
        "\n",
        "def compute_accuracy(X, Y, weights):\n",
        "    \"\"\"\n",
        "    Compute predictions for X given the weights and then find out how accurate are the predictions\n",
        "    by comparing with y\n",
        "\n",
        "    :param X: Input Tensors to be predicted\n",
        "    :param Y: Ground Truth Labels\n",
        "    :param weights: Weight (Model)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    N = X.shape[1]\n",
        "    A3 = predict(X, weights)\n",
        "    res = A3 == Y\n",
        "    accuracy = np.sum(res.all(axis=0)) / N\n",
        "    print(\"Accuracy: \" + str(accuracy))\n",
        "    return accuracy\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDi1pb_ti4LR"
      },
      "source": [
        "## Train on sklearn MNIST data \n",
        "input = 8X8 pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwo7pK_ti4LR",
        "outputId": "93427443-7387-44e0-fc16-6cfe11cb641b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Preparing the digits dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "\n",
        "X = digits.data\n",
        "\n",
        "# One hot encoding of target (Y)\n",
        "Y = np_utils.to_categorical(digits.target, 10)\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Transposing the input/output data as implementation expects them to be\n",
        "# In Library implementations its usually the weights which are transposed\n",
        "\n",
        "X_train = np.transpose(X_train)\n",
        "X_test = np.transpose(X_test)\n",
        "Y_train = np.transpose(Y_train)\n",
        "Y_test = np.transpose(Y_test)\n",
        "\n",
        "\n",
        "n_x = 64\n",
        "n_h1 = 40\n",
        "n_h2 = 20\n",
        "n_y = 10\n",
        "layers_dims = (n_x, n_h1, n_h2, n_y)\n",
        "\n",
        "weights = fit(X_train, Y_train, layers_dims = (n_x, n_h1, n_h2, n_y), num_iterations = 10000, print_cost=True)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 6.927758213084503\n",
            "Cost after iteration 100: 3.2734321092361194\n",
            "Cost after iteration 200: 3.272013882242851\n",
            "Cost after iteration 300: 3.27077045635114\n",
            "Cost after iteration 400: 3.2694525244001924\n",
            "Cost after iteration 500: 3.267752714442069\n",
            "Cost after iteration 600: 3.2650325127930517\n",
            "Cost after iteration 700: 3.25941325638326\n",
            "Cost after iteration 800: 3.2429323993325827\n",
            "Cost after iteration 900: 3.162153346617937\n",
            "Cost after iteration 1000: 2.7514846262985024\n",
            "Cost after iteration 1100: 2.2730131266448064\n",
            "Cost after iteration 1200: 1.9297770181872405\n",
            "Cost after iteration 1300: 1.5725724533918406\n",
            "Cost after iteration 1400: 1.3669183679538262\n",
            "Cost after iteration 1500: 1.2266482390271327\n",
            "Cost after iteration 1600: 1.1015916303678444\n",
            "Cost after iteration 1700: 0.9847125396257518\n",
            "Cost after iteration 1800: 0.8746599979896628\n",
            "Cost after iteration 1900: 0.7741030663294607\n",
            "Cost after iteration 2000: 0.6834265317759509\n",
            "Cost after iteration 2100: 0.6021422148264031\n",
            "Cost after iteration 2200: 0.5316391102076774\n",
            "Cost after iteration 2300: 0.47335496240994646\n",
            "Cost after iteration 2400: 0.4254223312792669\n",
            "Cost after iteration 2500: 0.38370427030917637\n",
            "Cost after iteration 2600: 0.35067634484249594\n",
            "Cost after iteration 2700: 0.32350769569384324\n",
            "Cost after iteration 2800: 0.3007812834861828\n",
            "Cost after iteration 2900: 0.28112302104531833\n",
            "Cost after iteration 3000: 0.2639074653565234\n",
            "Cost after iteration 3100: 0.2487729849056613\n",
            "Cost after iteration 3200: 0.23524486113931412\n",
            "Cost after iteration 3300: 0.22277045315154528\n",
            "Cost after iteration 3400: 0.2112800222956805\n",
            "Cost after iteration 3500: 0.20065768894395997\n",
            "Cost after iteration 3600: 0.19075788924194859\n",
            "Cost after iteration 3700: 0.18143451173844777\n",
            "Cost after iteration 3800: 0.17259179080353712\n",
            "Cost after iteration 3900: 0.16425595095751866\n",
            "Cost after iteration 4000: 0.15631189637352125\n",
            "Cost after iteration 4100: 0.14876585256988137\n",
            "Cost after iteration 4200: 0.14153035937165445\n",
            "Cost after iteration 4300: 0.13455591772846315\n",
            "Cost after iteration 4400: 0.12799319444877802\n",
            "Cost after iteration 4500: 0.12187146498460397\n",
            "Cost after iteration 4600: 0.11608502300056275\n",
            "Cost after iteration 4700: 0.11044128621228391\n",
            "Cost after iteration 4800: 0.10513897818309477\n",
            "Cost after iteration 4900: 0.10017739538523841\n",
            "Cost after iteration 5000: 0.0955752503600815\n",
            "Cost after iteration 5100: 0.09128228743903277\n",
            "Cost after iteration 5200: 0.08726065276376693\n",
            "Cost after iteration 5300: 0.0834069425086436\n",
            "Cost after iteration 5400: 0.07980851764486874\n",
            "Cost after iteration 5500: 0.07642963666232234\n",
            "Cost after iteration 5600: 0.07324870681580398\n",
            "Cost after iteration 5700: 0.07026942348255472\n",
            "Cost after iteration 5800: 0.06746033154546999\n",
            "Cost after iteration 5900: 0.06484614003400593\n",
            "Cost after iteration 6000: 0.06238632975905933\n",
            "Cost after iteration 6100: 0.060073615382754665\n",
            "Cost after iteration 6200: 0.05788874578958239\n",
            "Cost after iteration 6300: 0.05581590024684355\n",
            "Cost after iteration 6400: 0.0538456697197061\n",
            "Cost after iteration 6500: 0.05198022910241192\n",
            "Cost after iteration 6600: 0.05021024928772913\n",
            "Cost after iteration 6700: 0.048528122120043926\n",
            "Cost after iteration 6800: 0.046931367522447566\n",
            "Cost after iteration 6900: 0.04542053344644546\n",
            "Cost after iteration 7000: 0.04397141166629081\n",
            "Cost after iteration 7100: 0.04260277461327019\n",
            "Cost after iteration 7200: 0.041293789542649624\n",
            "Cost after iteration 7300: 0.0400351053132484\n",
            "Cost after iteration 7400: 0.03883946965052854\n",
            "Cost after iteration 7500: 0.03769930923838002\n",
            "Cost after iteration 7600: 0.03660276747666619\n",
            "Cost after iteration 7700: 0.03555868272440337\n",
            "Cost after iteration 7800: 0.03455336673008045\n",
            "Cost after iteration 7900: 0.03359526654198402\n",
            "Cost after iteration 8000: 0.03268178767191665\n",
            "Cost after iteration 8100: 0.031791658226935846\n",
            "Cost after iteration 8200: 0.030945912876168435\n",
            "Cost after iteration 8300: 0.030140901599732723\n",
            "Cost after iteration 8400: 0.029360166680560726\n",
            "Cost after iteration 8500: 0.028614941936819505\n",
            "Cost after iteration 8600: 0.027901049262292338\n",
            "Cost after iteration 8700: 0.027210709936194954\n",
            "Cost after iteration 8800: 0.026544202636827115\n",
            "Cost after iteration 8900: 0.025905886646726776\n",
            "Cost after iteration 9000: 0.02529087771283756\n",
            "Cost after iteration 9100: 0.02469280804245961\n",
            "Cost after iteration 9200: 0.024117909879994574\n",
            "Cost after iteration 9300: 0.023562034167862955\n",
            "Cost after iteration 9400: 0.023026208310163294\n",
            "Cost after iteration 9500: 0.02250456686147803\n",
            "Cost after iteration 9600: 0.022005306331608066\n",
            "Cost after iteration 9700: 0.021511518715627873\n",
            "Cost after iteration 9800: 0.021036803264899446\n",
            "Cost after iteration 9900: 0.020579615702406392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9LsOx5ui4LR"
      },
      "source": [
        "## Predict and compute accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg8R82AUi4LR",
        "outputId": "b0f19d47-d99c-44d0-8090-47579d5810f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy_train = compute_accuracy(X_train, Y_train, weights)\n",
        "accuracy_test = compute_accuracy(X_test, Y_test, weights)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9992576095025983\n",
            "Accuracy: 0.9511111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQJHBuohi4LR"
      },
      "source": [
        "## TBD: Show some images for which the model makes errors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89wbZnhpnykK",
        "outputId": "233e904c-7148-484b-eb3d-d9ee5481f08c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "print(f'len of data = {len(digits.data)}')\n",
        "print(len(images_and_labels))\n",
        "plt.figure(figsize=(8,6))\n",
        "for index, (image, label) in enumerate(images_and_labels[1:6]):\n",
        "    plt.subplot(1, 6, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('Training: %i' % label)\n",
        "\n",
        "print(digits.keys())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of data = 1797\n",
            "1797\n",
            "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytTN3_Hno9tY",
        "outputId": "7e65856b-d54d-499e-f3ec-58eb897c4214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print(digits['DESCR'])\n",
        "print(len(digits['data'][0]))\n",
        "# 64\n",
        "print(len(digits['data']))\n",
        "# 1797\n",
        "print(digits['data'][0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "1797\n",
            "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
            " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
            "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
            "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77zhd2Cmpkwr",
        "outputId": "4c2750e9-6a92-4d86-c7ec-4bfbda32bfe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# X_test[:,[1]]\n",
        "ax_test = X_test[:, [19, 55, 300]]\n",
        "ay_test = Y_test[:, [19, 55, 300]]\n",
        "print(ay_test)\n",
        "\n",
        "predict(ax_test, weights)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUKXUdaYi4LR"
      },
      "source": [
        "## TBD: Generalize the Fit function to accept any number of layers.\n",
        "Currently fit is hardcoded for a 3 Layer neural network, with Relu Activation for L1 and L2 and Sigmoid activation for L3. Generalize the fit  to accept any layer and activation functions. You can use following sequence of steps to achieve this.\n",
        "\n",
        "### TBD 1.  Change layer_dims to layer_confs, previously it was just a list of numbers but now it will be a list of tuples (number_of_nodes, activation_function). E.g layer_confs for existing NN will be. \\[ (64, None), (40, relu), (20, relu), (10, sigmoid) \\] \n",
        "\n",
        "### TBD 2.  Modify init_weights function to initilaize a general weights setting using layer_confs\n",
        "\n",
        "### TBD 3.  Modify fit function to use layer_confs to loop to go forward and backward.\n",
        "\n",
        "### TBD 4.  Use this general function to test the existing 3 layer configuration and see if you get same accuracy results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx-cBsEYsaL5"
      },
      "source": [
        "layer_confs = [ (64, None), (40, relu), (20, relu), (10, sigmoid) ]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYocmthii4LR"
      },
      "source": [
        "def init_weights2(layer_confs):\n",
        "    \"\"\"\n",
        "\n",
        "    Initialize weights tensor\n",
        "\n",
        "    :param n_x: size of the input layer\n",
        "    :param n_h1: size of the hidden layer 1\n",
        "    :param n_h2: size of the hidden layer 2\n",
        "    :param n_y: size of the output layer\n",
        "\n",
        "    :return:     weights -- python dictionary containing initialized weights:\n",
        "                    W1 -- weight matrix of shape (n_h1, n_x)\n",
        "                    b1 -- bias vector of shape (n_h1, 1)\n",
        "                    W2 -- weight matrix of shape (n_h2, n_h1)\n",
        "                    b2 -- bias vector of shape (n_h2, 1)\n",
        "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
        "                    b3 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = np.random.randn(n_h1, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h1, 1))\n",
        "    W2 = np.random.randn(n_h2, n_h1) * 0.01\n",
        "    b2 = np.zeros((n_h2, 1))\n",
        "    W3 = np.random.randn(n_y, n_h2) * 0.01\n",
        "    b3 = np.zeros((n_y, 1))\n",
        "\n",
        "    weights = { \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 }\n",
        "\n",
        "    return weights\n",
        "\n",
        "    print(weights)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIAQ1qLXso8t"
      },
      "source": [
        "def fit(X, Y, layers_conf, learning_rate=0.0075, num_iterations=10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Trains a 3 layer neural network\n",
        "\n",
        "    :param X: input features\n",
        "    :param Y: output labels\n",
        "    :param layers_dims: number of nodes in each layer ( inputs, number of neurons in each hidden layer, output)\n",
        "    :param learning_rate: \n",
        "    :param num_iterations:\n",
        "    :param print_cost: \n",
        "\n",
        "    :return: Weights and Bias of the trained network. \n",
        "\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []  # to keep track of the cost\n",
        "    m = X.shape[1]  # number of examples\n",
        "    \n",
        "    (n_x, n_h1, n_h2, n_y) = layers_conf\n",
        "\n",
        "    weights = init_weights(n_x, n_h1, n_h2, n_y)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: (LINEAR -> RELU) * 2 -> LINEAR -> SIGMOID.\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from weights\n",
        "        W1 = weights[\"W1\"]\n",
        "        b1 = weights[\"b1\"]\n",
        "        W2 = weights[\"W2\"]\n",
        "        b2 = weights[\"b2\"]\n",
        "        W3 = weights[\"W3\"]\n",
        "        b3 = weights[\"b3\"]\n",
        "\n",
        "\n",
        "        A1, Z1  = step_forward(X, W1, b1, relu)\n",
        "        A2, Z2  = step_forward(A1, W2, b2, relu)\n",
        "        A3, Z3  = step_forward(A2, W3, b3, sigmoid)\n",
        "\n",
        "        cost = compute_cost(A3, Y)\n",
        "\n",
        "        # Initializing backward propagation\n",
        "        dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3))\n",
        "\n",
        "        # gradient descent through back propagation\n",
        "        dA2, dW3, db3 = step_backward(dA3, Z3, A2, W3, b3, sigmoid_backward)\n",
        "        dA1, dW2, db2 = step_backward(dA2, Z2, A1, W2, b2, relu_backward)\n",
        "        dA0, dW1, db1 = step_backward(dA1, Z1, X, W1, b1,  relu_backward)\n",
        "\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        grads['dW3'] = dW3\n",
        "        grads['db3'] = db3\n",
        "\n",
        "        # Update weights.\n",
        "        weights = update_weights(weights, grads, learning_rate)\n",
        "\n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HITy4_ELt_BO",
        "outputId": "34c71038-35be-4120-8b50-3b4872cde352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Preparing the digits dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "\n",
        "X = digits.data\n",
        "\n",
        "# One hot encoding of target (Y)\n",
        "Y = np_utils.to_categorical(digits.target, 10)\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Transposing the input/output data as implementation expects them to be\n",
        "# In Library implementations its usually the weights which are transposed\n",
        "\n",
        "X_train = np.transpose(X_train)\n",
        "X_test = np.transpose(X_test)\n",
        "Y_train = np.transpose(Y_train)\n",
        "Y_test = np.transpose(Y_test)\n",
        "\n",
        "\n",
        "n_x = (64, None)\n",
        "n_h1 = (40, relu)\n",
        "n_h2 = (20, relu)\n",
        "n_y = (10, sigmoid)\n",
        "#layers_conf = [n_x, n_h1, n_h2, n_y]\n",
        "layers_conf = [ (64, None), (40, relu), (20, relu), (10, sigmoid) ]\n",
        "\n",
        "print (layers_conf)\n",
        "\n",
        "#weights = fit(X_train, Y_train, layers_dims = (n_x, n_h1, n_h2, n_y), num_iterations = 10000, print_cost=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(64, None), (40, <function relu at 0x7f35fb45a840>), (20, <function relu at 0x7f35fb45a840>), (10, <function sigmoid at 0x7f35fb45a8c8>)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}